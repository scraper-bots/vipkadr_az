#!/usr/bin/env python3
"""
VipKadr.az Candidate Scraper - Scrapes all candidate data from job listings
"""

import asyncio
import time
from datetime import datetime
from vipkadr_scraper import VipKadrScraper

async def scrape_all_candidates():
    """Scrape all candidates from all pages by default"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    print("ğŸš€ VipKadr.az Candidate Scraper")
    print("=" * 50)
    print(f"ğŸ“… Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("ğŸ¯ Scraping ALL candidates from ALL pages...")
    
    start_time = time.time()
    
    # Auto-detect last page or use high default
    async with VipKadrScraper(max_concurrent=15, delay=0.2) as scraper:
        try:
            # Get all candidate URLs from all pages (1 to 100 to ensure we get everything)
            print("\nğŸ“‹ Collecting candidate URLs from all pages...")
            job_urls = await scraper.scrape_all_pages(start_page=1, end_page=100)
            
            if not job_urls:
                print("âŒ No candidate URLs found")
                return
                
            print(f"âœ… Found {len(job_urls)} unique candidate listings")
            
            # Scrape all candidate details
            print(f"\nğŸ” Extracting detailed information for {len(job_urls)} candidates...")
            await scraper.scrape_all_jobs(job_urls)
            
            if not scraper.scraped_data:
                print("âŒ No candidate details were scraped")
                return
            
            # Save only latest results (most detailed version)
            print(f"\nğŸ’¾ Saving candidate data...")
            scraper.save_to_csv("vipkadr_candidates.csv")
            scraper.save_to_json("vipkadr_candidates.json")
            
            end_time = time.time()
            duration = end_time - start_time
            
            # Results summary
            print(f"\nğŸ‰ SCRAPING COMPLETED!")
            print("=" * 50)
            print(f"â±ï¸  Total time: {duration:.2f} seconds ({duration/60:.1f} minutes)")
            print(f"ğŸ“Š Candidates scraped: {len(scraper.scraped_data)}")
            print(f"âš¡ Average time per candidate: {duration/len(scraper.scraped_data):.2f}s")
            
            # Contact info statistics
            with_phone = sum(1 for job in scraper.scraped_data if job.get('phone'))
            with_email = sum(1 for job in scraper.scraped_data if job.get('email'))
            
            print(f"\nğŸ“ Contact Information:")
            print(f"   â€¢ Candidates with phone numbers: {with_phone} ({with_phone/len(scraper.scraped_data)*100:.1f}%)")
            print(f"   â€¢ Candidates with email addresses: {with_email} ({with_email/len(scraper.scraped_data)*100:.1f}%)")
            
            print(f"\nğŸ“ Output Files:")
            print(f"   â€¢ vipkadr_candidates.csv")
            print(f"   â€¢ vipkadr_candidates.json")
            
        except KeyboardInterrupt:
            print("\nâš ï¸  Scraping interrupted by user")
            if scraper.scraped_data:
                print(f"ğŸ’¾ Saving {len(scraper.scraped_data)} candidates scraped so far...")
                scraper.save_to_csv("vipkadr_candidates.csv")
                scraper.save_to_json("vipkadr_candidates.json")
        except Exception as e:
            print(f"âŒ Error during scraping: {e}")
            import traceback
            traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(scrape_all_candidates())